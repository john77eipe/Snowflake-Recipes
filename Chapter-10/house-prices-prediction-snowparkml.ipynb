{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-30T21:46:22.233263Z","iopub.status.busy":"2024-03-30T21:46:22.232792Z","iopub.status.idle":"2024-03-30T21:46:23.572322Z","shell.execute_reply":"2024-03-30T21:46:23.570893Z","shell.execute_reply.started":"2024-03-30T21:46:22.233227Z"},"trusted":true},"outputs":[],"source":["# Generic\n","import io\n","\n","# Joblib\n","import joblib\n","\n","# Snowpark ML\n","import snowflake.ml.modeling.preprocessing as snowml\n","from snowflake.snowpark.functions import sproc\n","from snowflake.snowpark import Session"]},{"cell_type":"markdown","metadata":{},"source":["#### Prerequisites\n","1.\tDownload data file from https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n","2.\tDownload Kaggle data set and upload it in snowflake\n","a.\tYou can use the snowflake snowsight (web UI) to upload a dataset and create a table in a single step.\n","b.\tIn this solution, the table is created with the name HOUSE_PRICES_RAW_DATA for the train data set and HOUSE_PRICES_TEST_DATA for the test data set.\n","3.\tCreate Kaggle account or Google Collab account or you may setup Jypter to run locally.\n","4. If you had already followed through the previous chapters, you would already have a database called \"RAW\" and schema \"RETAIL\". If not, the database and schema used in this example is RAW.RETAIL and it needs to be changed as appropritate.\n","5. The example also assumes you have a database called \"COMMONS\" with schema called \"UTILS\" which is used for storing models and stored procedures.\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Create session and load data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T21:46:45.042759Z","iopub.status.busy":"2024-03-30T21:46:45.042309Z","iopub.status.idle":"2024-03-30T21:46:49.598788Z","shell.execute_reply":"2024-03-30T21:46:49.597482Z","shell.execute_reply.started":"2024-03-30T21:46:45.042721Z"},"trusted":true},"outputs":[],"source":["connection_parameters = {\n","    \"account\": \"\",\n","    \"user\": \"\",\n","    \"password\": \"\",\n","    \"warehouse\": \"\", # optional\n","    \"database\":\"COMMONS\",\n","    \"schema\":\"UTILS\"\n","}  \n","\n","session = Session.builder.configs(connection_parameters).create()  "]},{"cell_type":"markdown","metadata":{},"source":["#### Add packages which are needed within your functions\n","\n","As you see, there are minimal packages needed as we are utilizing Snowpark ML api to perform the preprocessing, model creation and management"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["session.add_packages('snowflake-snowpark-python', 'snowflake-ml-python')"]},{"cell_type":"markdown","metadata":{},"source":["#### Create a stage to store your model"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(status='Stage area SNOWMODELS successfully created.')]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["query = \"create or replace stage commons.utils.snowmodels\" +\\\n","        \" directory = (enable = true)\" +\\\n","        \" copy_options = (on_error='skip_file')\"\n","        \n","session.sql(query).collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Input variables\n","\n","We define the training table name, source columns and the target column available within the table."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["training_table = 'RAW.RETAIL.HOUSE_PRICES_RAW_DATA'\n","src_cols = [\"BLDGTYPE\", \"OVERALLCOND\", \"MSSUBCLASS\", \"MSZONING\",\"LOTAREA\", \"LOTCONFIG\", \"YEARBUILT\", \"FOUNDATION\"]\n","target_col = 'SALEPRICE'"]},{"cell_type":"markdown","metadata":{},"source":["Check if the table has been read properly by displaying the count."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df = session.table(training_table).limit(1000).select(src_cols)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["['BLDGTYPE',\n"," 'OVERALLCOND',\n"," 'MSSUBCLASS',\n"," 'MSZONING',\n"," 'LOTAREA',\n"," 'LOTCONFIG',\n"," 'YEARBUILT',\n"," 'FOUNDATION']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"markdown","metadata":{},"source":["#### Create stored procedure to train and upload the model\n","\n","We create a function that will be deployed as a stored procedure to train a random forest regression model on the sample dataset to predict house prices. \n","We will use Snowpark ML features instead of sklearn and the model is saved using the Snowpark MLOps features instead of directly loading it into a stage using joblib. \n","The response from this procedure is important as it is later used to retrieve the features used by the model.\n","\n","Note that I did not create a named procedure, instead went with a temporary procedure (snowflake assigns a random name and associates it with the local function name). The temporary stored procedure will only be available within the current snowflake session. Once closed, you would not be able to call or use it from another session like from snowflake worksheets or a different jypter worksheet.\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def save_file(session, model, path):\n","    \"\"\"\n","    The function save_file is responsible for saving a machine learning model to a specified path using the Snowflake Python connector.\n","\n","    Args:\n","        session: A Snowflake session object that represents the connection to the Snowflake database.\n","        model: The machine learning model object that needs to be saved.\n","        path: The path where the model file will be saved.\n","\n","    Returns:\n","        str: success message\n","    \"\"\"\n","\n","    # takes the model object and serializes it using the joblib.dump() function \n","    # creates an input stream using io.BytesIO() to store the serialized model\n","    input_stream = io.BytesIO()\n","    joblib.dump(model, input_stream)\n","    \n","    # the serialized model is uploaded to the specified path using the upload_stream() method of the Snowflake connection cursor.\n","    session._conn._cursor.upload_stream(input_stream, path)\n","    return \"successfully created file: \" + path\n","\n","@sproc(replace=True)\n","def train_model(session: Session, training_table: str, src_cols: list, target_col: str) -> str:\n","    \"\"\"\n","    This function trains a random forest regression model to predict house prices. \n","    The function performs data preprocessing, including one-hot encoding of categorical variables, cleaning column names, and splitting the dataset into training and validation sets. It then trains the random forest regression model using the training set and predicts house prices for the validation set. \n","    The function saves the trained model to a file and returns a JSON string containing information about the model.\n","    \n","    Args:\n","        session (snowflake.snowpark.Session): A Snowflake session object.\n","        training_table (str): The name of the training table in Snowflake.\n","        src_cols (list): A list of source columns to use for training the model.\n","        target_col (str): The target column to predict.\n","\n","    Returns:\n","        str: JSON string containing information about the model's feature details, feature importance, and mean absolute percentage error.\n","    \"\"\"\n","    import logging\n","    logger = logging.getLogger(\"train_model\")\n","\n","    # load the raw data from the training table into a snowflake DataFrame and extract only needed columns\n","    raw_data = session.table(training_table)\n","    cols = src_cols\n","    cols.append(target_col)\n","    train_dataset = raw_data[cols]\n","    \n","    # identify the categorical columns in the source columns and Perform one-hot encoding on the categorical columns.\n","     \n","    # Get a list of all categorical columns\n","    categorical_columns = [column for column, dtype in train_dataset.dtypes if dtype.startswith(\"string\")]\n","\n","    def replace_characters(x: str, regex_string: str = '[^a-zA-Z,_]') -> str:\n","        \"\"\"Removes regex patterns from string.\n","    \n","        Args:\n","            x (str) : Target string to make replacement.\n","            regex_string (str) : Regex string to remove from x.\n","    \n","        Returns\n","            str\n","    \n","        \"\"\"\n","        import re\n","        if isinstance(x, str):\n","            regex = re.compile(regex_string)\n","            return regex.sub('', x).upper()\n"," \n"," \n","    # loop through the column names and rename them\n","    for column in train_dataset.columns:\n","        new_col_name = replace_characters(column)\n","        train_dataset = train_dataset.withColumnRenamed(column, new_col_name)\n","    \n","    # the above step was needed for OneHotEncoder to work\n","    OH_encoder = snowml.OneHotEncoder(input_cols=categorical_columns, output_cols=categorical_columns, drop_input_cols=True)\n","    train_df = OH_encoder.fit(train_dataset).transform(train_dataset)\n","\n","    # the below code is needed because the encoding adds unwanted quotes and some special characters into\n","    # the new column names\n","    import re\n","    for column in train_df.columns:\n","        tempColName = re.sub(r'\"', '',  column) # first remove quotes\n","        tempColName = re.sub(r'[^_a-zA-Z0-9]', '_', tempColName) # now remove all other characters and replace with underscore\n","        train_df = train_df.with_column_renamed(column, tempColName)\n","\n","    XY_train, XY_test = train_df.random_split(weights=[0.8, 0.2])\n","\n","    from snowflake.ml.modeling.ensemble.random_forest_regressor import RandomForestRegressor\n","\n","    model_RFR = RandomForestRegressor(n_estimators=10, label_cols=target_col)\n","    model_RFR.fit(XY_train)\n","\n","    result = model_RFR.predict(XY_test)\n","\n","    from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n","\n","\n","    # save the trained model into Snowflake Registery.\n","\n","    # create a dictionary containing information about the models feature details, feature importance, and mean absolute percentage error.\n","    model_info = dict()\n","    model_info['model_name']=\"houseprice_estimator\"\n","    model_info['model_features'] = train_df.columns\n","    model_info['mean_absolute_percentage_error']= mean_absolute_percentage_error(df=result, \n","                                            y_true_col_names=\"SALEPRICE\", \n","                                            y_pred_col_names=\"OUTPUT_SALEPRICE\")\n","    \n","    logger.info('Saving Model into Stage')\n","    path = save_file(session, model_RFR, \"@SNOWMODELS/houseprice_estimator.joblib\")\n","    logger.info('Saved Model:'+path)\n","\n","    import json\n","    return json.dumps(model_info)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# invoke the procedure to create and upload the model\n","model_results = train_model(training_table, src_cols, target_col)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import json\n","model_results_json = json.loads(model_results)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","    \"model_name\": \"houseprice_estimator\",\n","    \"model_features\": [\n","        \"BLDGTYPE_1FAM\",\n","        \"BLDGTYPE_2FMCON\",\n","        \"BLDGTYPE_DUPLEX\",\n","        \"BLDGTYPE_TWNHS\",\n","        \"BLDGTYPE_TWNHSE\",\n","        \"MSZONING__C__ALL__\",\n","        \"MSZONING_FV\",\n","        \"MSZONING_RH\",\n","        \"MSZONING_RL\",\n","        \"MSZONING_RM\",\n","        \"LOTCONFIG_CORNER\",\n","        \"LOTCONFIG_CULDSAC\",\n","        \"LOTCONFIG_FR2\",\n","        \"LOTCONFIG_FR3\",\n","        \"LOTCONFIG_INSIDE\",\n","        \"FOUNDATION_BRKTIL\",\n","        \"FOUNDATION_CBLOCK\",\n","        \"FOUNDATION_PCONC\",\n","        \"FOUNDATION_SLAB\",\n","        \"FOUNDATION_STONE\",\n","        \"FOUNDATION_WOOD\",\n","        \"OVERALLCOND\",\n","        \"MSSUBCLASS\",\n","        \"LOTAREA\",\n","        \"YEARBUILT\",\n","        \"SALEPRICE\"\n","    ],\n","    \"mean_absolute_percentage_error\": 0.20127441942975144\n","}\n"]}],"source":["print(json.dumps(model_results_json, indent=4))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["['BLDGTYPE_1FAM',\n"," 'BLDGTYPE_2FMCON',\n"," 'BLDGTYPE_DUPLEX',\n"," 'BLDGTYPE_TWNHS',\n"," 'BLDGTYPE_TWNHSE',\n"," 'MSZONING__C__ALL__',\n"," 'MSZONING_FV',\n"," 'MSZONING_RH',\n"," 'MSZONING_RL',\n"," 'MSZONING_RM',\n"," 'LOTCONFIG_CORNER',\n"," 'LOTCONFIG_CULDSAC',\n"," 'LOTCONFIG_FR2',\n"," 'LOTCONFIG_FR3',\n"," 'LOTCONFIG_INSIDE',\n"," 'FOUNDATION_BRKTIL',\n"," 'FOUNDATION_CBLOCK',\n"," 'FOUNDATION_PCONC',\n"," 'FOUNDATION_SLAB',\n"," 'FOUNDATION_STONE',\n"," 'FOUNDATION_WOOD',\n"," 'OVERALLCOND',\n"," 'MSSUBCLASS',\n"," 'LOTAREA',\n"," 'YEARBUILT']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["model_features = model_results_json['model_features']\n","model_features.remove(target_col)\n","model_features"]},{"cell_type":"markdown","metadata":{},"source":["### Unit test the model using a python stored procedure\n","\n","Like above, I'm using a anonymous (temporary) procedure. All operations happens within the proc which includes - reading test data from snowflake, loading the pre-trained model from file, encodes categorical variables in the test dataset before using it to test the model and finally returning the lsit of predicted values.\n","\n","Do not consider this as a best practice for unit testing and only consider this as a quick and dirty way to test your model."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","\n","session.add_import(\"@SNOWMODELS/houseprice_estimator.joblib\")  \n","\n","def read_file(filename):\n","    \"\"\"\n","    Reads the snowflake model from the stage location. \n","    You can think of this as mounting the stage location on a special path, which is local to the python container that is execuitng your code.\n","    The local path that has file mounted is available by reading the value of \"snowflake_import_directory\" system variable\n","    Args:\n","        filename (str): name of model file\n","\n","    Returns:\n","        binary: machine learning model\n","    \"\"\"\n","    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n","    if import_dir:\n","        with open(os.path.join(import_dir, filename), 'rb') as file:\n","                m = joblib.load(file)\n","                return m\n","\n","@sproc(replace=True)\n","def test_model(session: Session, test_table: str, src_cols: list) -> list:\n","    \"\"\"\n","    Tests the model\n","\n","    Args:\n","        session (snowflake.snowpark.Session): a Snowflake session object\n","        test_table (str): the name of the test table\n","        src_cols (list): a list of strings representing the names of the source columns\n","\n","    Returns:\n","        bool: success or failure\n","    \"\"\"\n","    \n","    import logging\n","    logger = logging.getLogger(\"test_model\")\n","\n","    model_RFR = read_file('houseprice_estimator.joblib')\n","\n","    if model_RFR is None:\n","        raise Exception('Unable to read model file')\n","    \n","    # load the raw data from the training table into a snowflake DataFrame and extract only needed columns\n","    raw_data = session.table(test_table)\n","    test_dataset = raw_data[src_cols]\n","\n","    # identify the categorical columns in the source columns and Perform one-hot encoding on the categorical columns.\n","    \n","    # Get a list of all categorical columns\n","    categorical_columns = [column for column, dtype in test_dataset.dtypes if dtype.startswith(\"string\")]\n","\n","    def replace_characters(x: str, regex_string: str = '[^a-zA-Z,_]') -> str:\n","        \"\"\"Removes regex patterns from string.\n","    \n","        Args:\n","            x (str) : Target string to make replacement.\n","            regex_string (str) : Regex string to remove from x.\n","    \n","        Returns\n","            str\n","    \n","        \"\"\"\n","        import re\n","        if isinstance(x, str):\n","            regex = re.compile(regex_string)\n","            return regex.sub('', x).upper()\n","\n","\n","    # loop through the column names and rename them\n","    for column in test_dataset.columns:\n","        new_col_name = replace_characters(column)\n","        test_dataset = test_dataset.withColumnRenamed(column, new_col_name)\n","    \n","    # the above step was needed for OneHotEncoder to work\n","    OH_encoder = snowml.OneHotEncoder(input_cols=categorical_columns, output_cols=categorical_columns, drop_input_cols=True)\n","    test_df = OH_encoder.fit(test_dataset).transform(test_dataset)\n","\n","    # the below code is needed because the encoding adds unwanted quotes and some special characters into\n","    # the new column names\n","    import re\n","    for column in test_df.columns:\n","        tempColName = re.sub(r'\"', '',  column) # first remove quotes\n","        tempColName = re.sub(r'[^_a-zA-Z0-9]', '_', tempColName) # now remove all other characters and replace with underscore\n","        test_df = test_df.with_column_renamed(column, tempColName)\n","\n","    # drop columns not available in the model\n","    unavailable_features_in_model = set(test_df.columns).difference(model_features)\n","    logger.info(unavailable_features_in_model)\n","\n","    for feature in unavailable_features_in_model:\n","        test_df = test_df.drop(feature)\n","    \n","    # add columns available in the model but not in the testdata (this method is not ideal but okay for quick poc)\n","    unavailable_feature_in_testdf = set(model_features).difference(test_df.columns)\n","\n","    for feature in unavailable_feature_in_testdf:\n","        test_df.with_column(feature, 0)\n","\n","\n","    test_df = test_df[model_features]\n","    \n","    # test_df['SALEPRICE_ESTIMATED']  = model_RFR.predict(test_df)\n","    \n","    tbl_ref = session.write_pandas(test_df, table_name='HOUSE_PRICES_TEST_DATA_RESULT', database='RAW', schema='RETAIL', auto_create_table=True, create_temp_table=False).collect()\n","    if tbl_ref is None:\n","        raise Exception('Error writing estimation results')\n","\n","    return test_df.columns\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["test_table =  'RAW.RETAIL.HOUSE_PRICES_TEST_DATA'"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["1459"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["session.table(test_table).count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_model(test_table, src_cols)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T21:48:18.114160Z","iopub.status.busy":"2024-03-30T21:48:18.113667Z","iopub.status.idle":"2024-03-30T21:48:18.457312Z","shell.execute_reply":"2024-03-30T21:48:18.455892Z","shell.execute_reply.started":"2024-03-30T21:48:18.114116Z"},"trusted":true},"outputs":[],"source":["session.close()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
